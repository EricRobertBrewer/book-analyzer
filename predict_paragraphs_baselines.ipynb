{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict_paragraphs_baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from classification import baselines, evaluation, ordinal, shared_parameters\n",
    "import folders\n",
    "import predict_paragraphs\n",
    "from sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'paragraph_tokens'\n",
    "subset_ratio = shared_parameters.DATA_SUBSET_RATIO\n",
    "subset_seed = shared_parameters.DATA_SUBSET_SEED\n",
    "min_len = shared_parameters.DATA_PARAGRAPH_MIN_LEN\n",
    "max_len = shared_parameters.DATA_PARAGRAPH_MAX_LEN\n",
    "min_tokens = shared_parameters.DATA_MIN_TOKENS\n",
    "categories_mode = shared_parameters.DATA_CATEGORIES_MODE\n",
    "return_overall = shared_parameters.DATA_RETURN_OVERALL\n",
    "inputs, Y, categories, category_levels, book_ids, books_df, _, _, categories_df = \\\n",
    "    bookcave.get_data({source},\n",
    "                      subset_ratio=subset_ratio,\n",
    "                      subset_seed=subset_seed,\n",
    "                      min_len=min_len,\n",
    "                      max_len=max_len,\n",
    "                      min_tokens=min_tokens,\n",
    "                      categories_mode=categories_mode,\n",
    "                      return_overall=return_overall,\n",
    "                      return_meta=True)\n",
    "text_source_tokens = list(zip(*inputs[source]))[0]\n",
    "len(text_source_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_locations = []\n",
    "predict_tokens = []\n",
    "predict_source_labels = []\n",
    "for text_i, source_tokens in enumerate(text_source_tokens):\n",
    "    book_id = book_ids[text_i]\n",
    "    asin = books_df[books_df['id'] == book_id].iloc[0]['asin']\n",
    "    category_labels = [bookcave.get_labels(asin, category) for category in categories]\n",
    "    if any(labels is None for labels in category_labels):\n",
    "        continue\n",
    "    for source_i, tokens in enumerate(source_tokens):\n",
    "        source_labels = [labels[source_i] for labels in category_labels]\n",
    "        if any(label == -1 for label in source_labels):\n",
    "            continue\n",
    "        predict_locations.append((text_i, source_i))\n",
    "        predict_tokens.append(tokens)\n",
    "        predict_source_labels.append(source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_true = np.array(predict_source_labels).transpose()\n",
    "Q_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "category_balanced_indices = [predict_paragraphs.get_balanced_indices(q_true, minlength=len(category_levels[j]), seed=seed)\n",
    "                             for j, q_true in enumerate(Q_true)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_zero_r(category_indices=None):\n",
    "    category_metrics_zero = []\n",
    "    print('ZeroR')\n",
    "    for j, category in enumerate(categories):\n",
    "        print()\n",
    "        print(category)\n",
    "        q_true = Q_true[j]\n",
    "        if category_indices is not None:\n",
    "            q_true = q_true[category_indices[j]]\n",
    "        q_pred_zero = [np.argmax(np.bincount(q_true, minlength=len(category_levels[j])))]*len(q_true)\n",
    "        confusion_zero, metrics_zero = evaluation.get_confusion_and_metrics(q_true, q_pred_zero)\n",
    "        print(confusion_zero)\n",
    "        print(metrics_zero[0])\n",
    "        category_metrics_zero.append(metrics_zero)\n",
    "    print()\n",
    "    print('Average')\n",
    "    metrics_avg_zero = [sum([metrics_zero[i] for metrics_zero in category_metrics_zero[:-1]])/(len(category_metrics_zero) - 1)\n",
    "                        for i in range(len(category_metrics_zero[0]))]\n",
    "    print(metrics_avg_zero[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_zero_r()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_zero_r(category_indices=category_balanced_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words (count-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = shared_parameters.TEXT_MAX_WORDS\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=identity,\n",
    "    tokenizer=identity,\n",
    "    analyzer='word',\n",
    "    token_pattern=None,\n",
    "    max_features=max_words,\n",
    "    norm='l2',\n",
    "    sublinear_tf=True)\n",
    "text_tokens = []\n",
    "for source_tokens in text_source_tokens:\n",
    "    all_tokens = []\n",
    "    for tokens in source_tokens:\n",
    "        all_tokens.extend(tokens)\n",
    "    text_tokens.append(all_tokens)\n",
    "X_w = vectorizer.fit_transform(text_tokens)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = shared_parameters.EVAL_TEST_SIZE  # b\n",
    "test_random_state = shared_parameters.EVAL_TEST_RANDOM_STATE\n",
    "Y_T = Y.transpose()  # (n, c)\n",
    "X_w_train, _, Y_train_T, _ = train_test_split(X_w, Y_T, test_size=test_size, random_state=test_random_state)\n",
    "Y_train = Y_train_T.transpose()  # (c, n * (1 - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict_baselines(create_model, P_w_predict, category_indices=None):\n",
    "    print()\n",
    "    print(create_model.__name__[7:])\n",
    "\n",
    "    # Fit models.\n",
    "    category_classifiers = []\n",
    "    for j, category in enumerate(categories):\n",
    "        y_train = Y_train[j]\n",
    "        k = len(category_levels[j])\n",
    "        classifiers = baselines.fit_ordinal(create_model, X_w_train, y_train, k)\n",
    "        category_classifiers.append(classifiers)\n",
    "\n",
    "    # Predict.\n",
    "    Q_w_pred = []\n",
    "    for j, classifiers in enumerate(category_classifiers):\n",
    "        k = len(category_levels[j])\n",
    "        if category_indices is not None:\n",
    "            q_w_pred = baselines.predict_ordinal(classifiers, P_w_predict[category_indices[j]], k)\n",
    "        else:\n",
    "            q_w_pred = baselines.predict_ordinal(classifiers, P_w_predict, k)\n",
    "        Q_w_pred.append(q_w_pred)\n",
    "\n",
    "    # Evaluate.\n",
    "    category_metrics = []\n",
    "    for j, category in enumerate(categories):\n",
    "        print()\n",
    "        print(category)\n",
    "        q_true = Q_true[j]\n",
    "        if category_indices is not None:\n",
    "            q_true = q_true[category_indices[j]]\n",
    "        q_w_pred = Q_w_pred[j]\n",
    "        confusion, metrics = evaluation.get_confusion_and_metrics(q_true, q_w_pred)\n",
    "        print(confusion)\n",
    "        print(metrics[0])\n",
    "        category_metrics.append(metrics)\n",
    "\n",
    "    # Average.\n",
    "    print()\n",
    "    print('Average')\n",
    "    metrics_avg = [sum([metrics[i] for metrics in category_metrics[:-1]])/(len(category_metrics) - 1)\n",
    "                   for i in range(len(category_metrics[0]))]\n",
    "    print(metrics_avg[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_models = [\n",
    "    baselines.create_k_nearest_neighbors,\n",
    "    baselines.create_linear_regression,\n",
    "    baselines.create_logistic_regression,\n",
    "    baselines.create_multinomial_naive_bayes,\n",
    "    baselines.create_random_forest,\n",
    "    baselines.create_svm]\n",
    "P_w_predict = vectorizer.transform(predict_tokens)\n",
    "for create_model in create_models:\n",
    "    predict_baselines(create_model, P_w_predict)\n",
    "    print()\n",
    "    print('Balanced')\n",
    "    predict_baselines(create_model, P_w_predict, category_indices=category_balanced_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_path = os.path.join(folders.MODELS_PATH, 'multi_layer_perceptron', 'ordinal', '32662682.h5')\n",
    "predict_paragraphs.load_model_and_evaluate(mlp_path,\n",
    "                                           P_w_predict,\n",
    "                                           Q_true,\n",
    "                                           categories,\n",
    "                                           overall_last=return_overall)\n",
    "predict_paragraphs.load_model_and_evaluate(mlp_path,\n",
    "                                           P_w_predict,\n",
    "                                           Q_true,\n",
    "                                           categories,\n",
    "                                           overall_last=return_overall,\n",
    "                                           category_indices=category_balanced_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
