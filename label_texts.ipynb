{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "import keras\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from classification import ordinal, paragraph_rnn\n",
    "import folders\n",
    "from sites.bookcave import bookcave\n",
    "from text import paragraph_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = os.path.join('models')\n",
    "GLOVE_100_PATH = os.path.join('..', '..', 'embeddings', 'glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_min_len = 250\n",
    "text_max_len = 7500\n",
    "only_categories = None\n",
    "if only_categories:\n",
    "    category_names = [bookcave.CATEGORY_NAMES[category_i] for category_i in only_categories]\n",
    "else:\n",
    "    category_names = bookcave.CATEGORY_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_inputs, Y, categories, category_levels, book_ids, books_df, _, _, categories_df =\\\n",
    "    bookcave.get_data({'text'},\n",
    "                      text_source='paragraphs',\n",
    "                      text_min_len=text_min_len,\n",
    "                      text_max_len=text_max_len,\n",
    "                      only_categories=only_categories,\n",
    "                      return_meta=True)\n",
    "paragraph_texts = paragraph_inputs['text']\n",
    "text_paragraphs, text_section_ids, text_sections = [], [], []\n",
    "for paragraphs, section_ids, sections in paragraph_texts:\n",
    "    text_paragraphs.append(paragraphs)\n",
    "    text_section_ids.append(section_ids)\n",
    "    text_sections.append(sections)\n",
    "len(text_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_to_index = {book_id: i for i, book_id in enumerate(book_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_category_rows = categories_df[categories_df['category'] == categories[0]]\n",
    "rating_names = [first_category_rows.iloc[i]['rating'] for i in range(len(first_category_rows))]\n",
    "rating_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_descriptions = [list(categories_df[categories_df['category'] == category]['description']) for category in categories]\n",
    "category_descriptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inputs, token_Y, _, _, token_book_ids, _, _, _, _ =\\\n",
    "    bookcave.get_data({'text'},\n",
    "                      text_source='tokens',\n",
    "                      text_min_len=text_min_len,\n",
    "                      text_max_len=text_max_len,\n",
    "                      only_categories=only_categories,\n",
    "                      return_meta=True)\n",
    "token_texts = [text for i, text in enumerate(token_inputs['text']) if token_book_ids[i] in book_ids]\n",
    "text_paragraph_tokens = [paragraph_tokens for paragraph_tokens, _ in token_texts]\n",
    "len(text_paragraph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_book_id_to_index = {book_id: i for i, book_id in enumerate(token_book_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_tokens = []\n",
    "for paragraph_tokens in text_paragraph_tokens:\n",
    "    all_tokens = []\n",
    "    for tokens in paragraph_tokens:\n",
    "        all_tokens.extend(tokens)\n",
    "    text_all_tokens.append(all_tokens)\n",
    "len(text_all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=identity,\n",
    "    tokenizer=identity,\n",
    "    analyzer='word',\n",
    "    token_pattern=None,\n",
    "    max_features=4096,\n",
    "    norm='l2',\n",
    "    sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(text_all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_chi2 = dict()\n",
    "top_n = 80\n",
    "features = np.array(vectorizer.get_feature_names())\n",
    "for category_i in range(len(categories)):\n",
    "    y = token_Y[category_i]\n",
    "    scores, pvals = chi2(X, y)\n",
    "    indices = np.argsort(scores)\n",
    "    for j, index in enumerate(indices[-top_n:]):\n",
    "        feature = features[index]\n",
    "#         score = scores[index]\n",
    "        score = top_n - j\n",
    "        if feature not in token_to_chi2.keys():\n",
    "            token_to_chi2[feature] = score\n",
    "        else:\n",
    "            token_to_chi2[feature] = max(token_to_chi2[feature], score)\n",
    "token_to_chi2['shit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_book_ids = [\n",
    "    # By total rating:\n",
    "    'temptation-island',\n",
    "    'forever-road',\n",
    "    'never-letting-go',\n",
    "    'collateral',\n",
    "    'dark-warrior-alliance-boxset-books-5-8',\n",
    "    'hell-on-earth',\n",
    "    'orb-station-zero',\n",
    "    'dette-chambers-death-journal',\n",
    "    'book-of-shadows',\n",
    "    'lustful-letters',\n",
    "    'masks-of-betrayal-2',\n",
    "    'in-pain-and-blood-2',\n",
    "    'werecat-the-rearing',\n",
    "    'hologram-the-seduction-of-samantha-bowman',\n",
    "    'highland-wolf-clan-the-reluctant-alpha',\n",
    "    'the-olympus-killer',\n",
    "    'collapse',\n",
    "    'torture-mom',\n",
    "    'more-than-friends-collection',\n",
    "    'to-betray-a-master',\n",
    "    # By ratio of total rating to # of paragraphs:\n",
    "    'theirs-for-the-night',\n",
    "    'come-away-with-me',\n",
    "    'the-wall',\n",
    "    'prayers-for-the-soul-of-a-dying-star',\n",
    "    'sweet-melissa-destination-unknown',\n",
    "    'ahrions-minions',\n",
    "    'diamond-hustle',\n",
    "    'ember-of-war-2',\n",
    "    'werecat-the-rearing',\n",
    "    'skyline-the-dragon-commander',\n",
    "    'lustful-lies',\n",
    "    'sprite-night',\n",
    "    'winter-thrillz',\n",
    "    'kismet',\n",
    "    'gettin-lucky',\n",
    "    'torture-mom',\n",
    "    'pleasuring-lady-pennington',\n",
    "    'cyborg-awakenings',\n",
    "    'ellies-encounter',\n",
    "    'borderline'\n",
    "]\n",
    "train_text_paragraphs = []\n",
    "train_text_section_ids = []\n",
    "train_text_sections = []\n",
    "train_text_category_labels = []\n",
    "train_text_paragraph_tokens = []\n",
    "train_text_paragraph_h = []\n",
    "for book_id in train_book_ids:\n",
    "    i = book_id_to_index[book_id]\n",
    "    train_text_paragraphs.append(text_paragraphs[i])\n",
    "    train_text_section_ids.append(text_section_ids[i])\n",
    "    train_text_sections.append(text_sections[i])\n",
    "    \n",
    "    # Load or create labels.\n",
    "    category_labels = []\n",
    "    asin = books_df[books_df['id'] == book_id].iloc[0]['asin']\n",
    "    for category in categories:\n",
    "        labels = bookcave.get_labels(asin, category)\n",
    "        if labels is None:\n",
    "            labels = [-1]*len(text_paragraphs[i])\n",
    "        category_labels.append(labels)\n",
    "    train_text_category_labels.append(category_labels)\n",
    "    \n",
    "    # Calculate maturity heuristic for each paragraph for each text.\n",
    "    token_i = token_book_id_to_index[book_id]\n",
    "    paragraph_tokens = text_paragraph_tokens[token_i]\n",
    "    train_text_paragraph_tokens.append(paragraph_tokens)\n",
    "    paragraph_h = []\n",
    "    for tokens in paragraph_tokens:\n",
    "        h = 0\n",
    "        token_set = set(tokens)\n",
    "        for token in token_set:\n",
    "            if token in token_to_chi2.keys():\n",
    "                h += token_to_chi2[token]\n",
    "        paragraph_h.append(h)\n",
    "    train_text_paragraph_h.append(paragraph_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_locations = []\n",
    "train_paragraph_h = []\n",
    "for text_i, paragraph_h in enumerate(train_text_paragraph_h):\n",
    "    for paragraph_i, h in enumerate(paragraph_h):\n",
    "        train_locations.append((text_i, paragraph_i))\n",
    "        train_paragraph_h.append(h)\n",
    "train_locations = np.array(train_locations)\n",
    "train_paragraph_h = np.array(train_paragraph_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_indices = np.argsort(train_paragraph_h)[::-1]\n",
    "train_text_paragraphs[train_locations[sorted_train_indices[0]][0]][train_locations[sorted_train_indices[0]][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [paragraph_i] = str\n",
    "train_text_paragraphs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [paragraph_i] = int\n",
    "train_text_section_ids[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [section_id] = str\n",
    "train_text_sections[0][train_text_section_ids[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [category_i], [paragraph_i] = int\n",
    "train_text_category_labels[0][0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [location_i]\n",
    "train_locations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [location_i]\n",
    "sorted_train_indices[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_predict = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rest of the cells in this section are only necessary if `do_predict == True`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 40000\n",
    "\n",
    "# Less than 1% of paragraphs contain more than 160 tokens.\n",
    "n_tokens = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_paragraph_tokens = []\n",
    "for paragraph_tokens in text_paragraph_tokens:\n",
    "    for tokens in paragraph_tokens:\n",
    "        all_paragraph_tokens.append(tokens)\n",
    "len(all_paragraph_tokens), all_paragraph_tokens[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(all_paragraph_tokens)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, embedding_matrix = paragraph_rnn.get_embedding(tokenizer, GLOVE_100_PATH, max_words)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "dense_size = 64\n",
    "train_emb = True\n",
    "\n",
    "models = []\n",
    "model_weights_fnames = []\n",
    "for category_i, levels in enumerate(category_levels):\n",
    "    category = categories[category_i]\n",
    "    n_classes = len(levels)\n",
    "    model, weights_fname = paragraph_rnn.create_model(category,\n",
    "                                                      n_classes,\n",
    "                                                      n_tokens,\n",
    "                                                      embedding_matrix,\n",
    "                                                      hidden_size,\n",
    "                                                      dense_size,\n",
    "                                                      train_emb=train_emb)\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    path = os.path.join(MODELS_PATH, weights_fname)\n",
    "    if os.path.exists(path):\n",
    "        model.load_weights(path)\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_array(sequence):\n",
    "    x = np.zeros((n_tokens,), dtype=np.int32)\n",
    "    if len(sequence) > n_tokens:\n",
    "        # Truncate center.\n",
    "        x[:n_tokens//2] = sequence[:n_tokens//2]\n",
    "        x[-n_tokens//2:] = sequence[-n_tokens//2:]\n",
    "    else:\n",
    "        # Pad beginning ('pre').\n",
    "        x[-len(sequence):] = sequence\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_predictions(x):\n",
    "    y_preds_ordinal = [model.predict([[x]], batch_size=1) for model in models]\n",
    "    y_preds = [ordinal.from_multi_hot_ordinal(y_pred_ordinal) for y_pred_ordinal in y_preds_ordinal]\n",
    "    return x, y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_overwrite = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate train index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_i = 0\n",
    "text_i, paragraph_i = train_locations[sorted_train_indices[train_i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_next_paragraph():\n",
    "    global train_i\n",
    "    global text_i\n",
    "    global paragraph_i\n",
    "    \n",
    "    if train_i == len(sorted_train_indices) - 1:\n",
    "        return False\n",
    "    \n",
    "    train_i += 1    \n",
    "    text_i, paragraph_i = train_locations[sorted_train_indices[train_i]]\n",
    "    return True\n",
    "\n",
    "\n",
    "def move_to_previous_paragraph():\n",
    "    global train_i\n",
    "    global text_i\n",
    "    global paragraph_i\n",
    "\n",
    "    if train_i == 0:\n",
    "        return False\n",
    "    \n",
    "    train_i -= 1\n",
    "    text_i, paragraph_i = train_locations[sorted_train_indices[train_i]]\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_html = widgets.HTML(value='')\n",
    "\n",
    "category_toggle_buttons = []\n",
    "for category_i, levels in enumerate(category_levels):\n",
    "    level_descriptions = category_descriptions[category_i]\n",
    "    options = []\n",
    "    description = category_names[category_i]\n",
    "    tooltips = []\n",
    "    for level_i, level in enumerate(levels):\n",
    "        options.append((rating_names[level_i], level_i))\n",
    "        split_levels = '\\n'.join(level.split('|'))\n",
    "        level_description = level_descriptions[level_i]\n",
    "        split_level_descriptions = '\\n'.join(level_description.split('|'))\n",
    "        tooltips.append('{}\\n\\n{}'.format(split_levels, split_level_descriptions))\n",
    "    toggle_buttons = widgets.ToggleButtons(\n",
    "        options=options,\n",
    "        description=description,\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltips=tooltips\n",
    "    )\n",
    "    category_toggle_buttons.append(toggle_buttons)\n",
    "\n",
    "\n",
    "def on_submit_button_clicked(button):\n",
    "    # Collect the answers.\n",
    "    for category_i, levels in enumerate(category_levels):\n",
    "        y = category_toggle_buttons[category_i].value\n",
    "        train_text_category_labels[text_i][category_i][paragraph_i] = y\n",
    "\n",
    "    moved = move_to_next_paragraph()\n",
    "    if not do_overwrite:\n",
    "        while moved and all([train_text_category_labels[text_i][category_i][paragraph_i] != -1\n",
    "                             for category_i in range(len(categories))]):\n",
    "            moved = move_to_next_paragraph()\n",
    "\n",
    "    if moved:\n",
    "        display_paragraph_interface()\n",
    "    else:\n",
    "        print('Finished training on {:d} books.'.format(len(train_paragraphs)))\n",
    "\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Submit the above values as the categorical maturity rating levels for this paragraph.\\nThen move on to the next paragraph.',\n",
    "    icon='check'\n",
    ")\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "\n",
    "def on_back_button_clicked(button):\n",
    "    moved_back = move_to_previous_paragraph()\n",
    "    if moved_back:\n",
    "        display_paragraph_interface()\n",
    "    else:\n",
    "        print('Cannot move to previous paragraph.')\n",
    "\n",
    "\n",
    "back_button = widgets.Button(\n",
    "    description='Back',\n",
    "    disabled=False,\n",
    "    button_style='warning',\n",
    "    tooltip='Move to the previous paragraph.'\n",
    ")\n",
    "back_button.on_click(on_back_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_paragraph_interface():\n",
    "    # Calculate meta data.\n",
    "    book_id = train_book_ids[text_i]\n",
    "    book_index = book_id_to_index[book_id]\n",
    "    book_title = books_df.iloc[book_index]['title']\n",
    "    book_authors = books_df.iloc[book_index]['authors']\n",
    "    book_y = Y[:, book_index]\n",
    "    \n",
    "    section_i = train_text_section_ids[text_i][paragraph_i]\n",
    "    sections = train_text_sections[text_i]\n",
    "    section = sections[section_i]\n",
    "    paragraph = train_text_paragraphs[text_i][paragraph_i]\n",
    "    \n",
    "    # Clear any previous output in this cell.\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    \n",
    "    # Print meta data.\n",
    "    c_width = 114\n",
    "    print('-'*c_width)\n",
    "    print('{} [book {:d} of {:d}]'.format(book_title, text_i + 1, len(train_book_ids)))\n",
    "    print('{}'.format(book_authors))\n",
    "    print()\n",
    "    print('Actual categorical rating levels:')\n",
    "    for category_i, level_i in enumerate(book_y):\n",
    "        category = categories[category_i]\n",
    "        level = category_levels[category_i][level_i]\n",
    "        print('  {:28}: {} ({})'.format(category, rating_names[level_i], level))\n",
    "    print('-'*c_width)\n",
    "    print()\n",
    "    print('What are the categorical maturity rating levels for this paragraph?')\n",
    "    \n",
    "    # Update toggle buttons from existing labels or predictions.\n",
    "    if all([train_text_category_labels[text_i][category_i][paragraph_i] != -1 for category_i in range(len(categories))]):\n",
    "        for category_i, toggle_buttons in enumerate(category_toggle_buttons):\n",
    "            toggle_buttons.value = train_text_category_labels[text_i][category_i][paragraph_i]\n",
    "    elif do_predict and book_id in token_book_id_to_index.keys():\n",
    "        token_book_index = token_book_id_to_index[book_id]\n",
    "        tokens = text_paragraph_tokens[token_book_index][paragraph_i]\n",
    "        train_sequence = tokenizer.texts_to_sequences([tokens])[0]\n",
    "        x_train = get_input_array(train_sequence)\n",
    "        y_preds = get_predictions(x_train)\n",
    "        for category_i, toggle_buttons in enumerate(category_toggle_buttons):\n",
    "            toggle_buttons.value = y_preds[category_i][0]\n",
    "    else:\n",
    "        for toggle_buttons in category_toggle_buttons:\n",
    "            toggle_buttons.value = 0\n",
    "    \n",
    "    # Display toggle buttons.\n",
    "    for toggle_buttons in category_toggle_buttons:\n",
    "        IPython.display.display(toggle_buttons)\n",
    "    \n",
    "    # Display submit button.\n",
    "    IPython.display.display(submit_button)\n",
    "    \n",
    "    print()\n",
    "    print('{} [section {:d} of {:d}]'.format(section, section_i + 1, len(sections)))\n",
    "    print()\n",
    "    print('[paragraph {:d} of {:d}]'.format(paragraph_i + 1, len(train_text_paragraphs[text_i])))\n",
    "    print('='*c_width)\n",
    "    paragraph_html.value = '<p style=\"font-size:large;margin-left:8em;max-width:36em;\">{}</p>'.format(paragraph)\n",
    "    IPython.display.display(paragraph_html)\n",
    "    print('='*c_width)\n",
    "    \n",
    "    # Display back button.\n",
    "    IPython.display.display(back_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_paragraph_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_i in range(len(train_text_category_labels)):\n",
    "    book_id = train_book_ids[text_i]\n",
    "    asin = books_df[books_df['id'] == book_id].iloc[0]['asin']\n",
    "\n",
    "    for category_i, category in enumerate(categories):\n",
    "        sections = train_text_sections[text_i]\n",
    "        section_ids = train_text_section_ids[text_i]\n",
    "        labels = train_text_category_labels[text_i][category_i]\n",
    "        bookcave.save_labels(asin, category, sections, section_ids, labels, force=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
