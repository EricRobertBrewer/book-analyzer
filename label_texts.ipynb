{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import keras\n",
    "import IPython\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from classification import ordinal, paragraph_rnn\n",
    "import folders\n",
    "from sites.bookcave import bookcave\n",
    "from text import paragraph_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = os.path.join('models')\n",
    "GLOVE_100_PATH = os.path.join('..', '..', 'embeddings', 'glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_min_len = 250\n",
    "only_categories = None\n",
    "if only_categories:\n",
    "    category_names = [bookcave.CATEGORY_NAMES[category_i] for category_i in only_categories]\n",
    "else:\n",
    "    category_names = bookcave.CATEGORY_NAMES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_inputs, Y, categories, category_levels, book_ids, books_df, _, _, categories_df =\\\n",
    "    bookcave.get_data({'text'},\n",
    "                      text_source='paragraphs',\n",
    "                      text_min_len=text_min_len,\n",
    "                      only_categories=only_categories,\n",
    "                      return_meta=True)\n",
    "paragraph_texts = paragraph_inputs['text']\n",
    "text_paragraphs, text_section_ids, text_sections = [], [], []\n",
    "for paragraphs, section_ids, sections in paragraph_texts:\n",
    "    text_paragraphs.append(paragraphs)\n",
    "    text_section_ids.append(section_ids)\n",
    "    text_sections.append(sections)\n",
    "text_paragraphs[0][0], text_sections[0][text_section_ids[0][0]]\n",
    "len(text_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_id_to_index = {book_id: i for i, book_id in enumerate(book_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_category_rows = categories_df[categories_df['category'] == categories[0]]\n",
    "rating_names = [first_category_rows.iloc[i]['rating'] for i in range(len(first_category_rows))]\n",
    "rating_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_descriptions = [list(categories_df[categories_df['category'] == category]['description']) for category in categories]\n",
    "category_descriptions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_inputs, _, _, _, label_book_ids, _, _, _, _ = \\\n",
    "    bookcave.get_data({'text'},\n",
    "                      text_source='labels',\n",
    "                      text_min_len=text_min_len,\n",
    "                      only_categories=only_categories,\n",
    "                      return_meta=True)\n",
    "text_category_labels = label_inputs['text']\n",
    "len(text_category_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_book_id_to_index = {book_id: i for i, book_id in enumerate(label_book_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_category_labels(paragraphs):\n",
    "    return [[-1]*len(paragraphs) for _ in range(len(categories))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_book_ids = ['borderline',\n",
    "                  'ellies-encounter',\n",
    "                  'cyborg-awakenings',\n",
    "                  'pleasuring-lady-pennington',\n",
    "                  'torture-mom',\n",
    "                  'gettin-lucky',\n",
    "                  'kismet',\n",
    "                  'winter-thrillz',\n",
    "                  'sprite-night',\n",
    "                  'lustful-lies',\n",
    "                  'skyline-the-dragon-commander',\n",
    "                  'werecat-the-rearing',\n",
    "                  'ember-of-war-2',\n",
    "                  'diamond-hustle',\n",
    "                  'ahrions-minions',\n",
    "                  'sweet-melissa-destination-unknown',\n",
    "                  'prayers-for-the-soul-of-a-dying-star',\n",
    "                  'the-wall',\n",
    "                  'come-away-with-me',\n",
    "                  'theirs-for-the-night']\n",
    "train_text_paragraphs = []\n",
    "train_text_section_ids = []\n",
    "train_text_sections = []\n",
    "train_text_category_labels = []\n",
    "for book_id in train_book_ids:\n",
    "    i = book_id_to_index[book_id]\n",
    "    train_text_paragraphs.append(text_paragraphs[i])\n",
    "    train_text_section_ids.append(text_section_ids[i])\n",
    "    train_text_sections.append(text_sections[i])\n",
    "    if book_id in label_book_id_to_index.keys():\n",
    "        j = label_book_id_to_index[book_id]\n",
    "        train_text_category_labels.append(text_category_labels[j])\n",
    "    else:\n",
    "        train_text_category_labels.append(create_category_labels(text_paragraphs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [paragraph_i] = str\n",
    "train_text_paragraphs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [paragraph_i] = int\n",
    "train_text_section_ids[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [section_id] = str\n",
    "train_text_sections[0][train_text_section_ids[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions: [text_i], [category_i], [paragraph_i] = int\n",
    "train_text_category_labels[0][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_predict = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rest of the cells in this section are only necessary if `do_predict == True`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inputs, _, _, _, token_book_ids, _, _, _, _ =\\\n",
    "    bookcave.get_data({'text'},\n",
    "                      text_source='tokens',\n",
    "                      text_min_len=text_min_len,\n",
    "                      only_categories=only_categories,\n",
    "                      return_meta=True)\n",
    "token_texts = token_inputs['text']\n",
    "text_paragraph_tokens = []\n",
    "for paragraph_tokens, _ in token_texts:\n",
    "    text_paragraph_tokens.append(paragraph_tokens)\n",
    "len(text_paragraph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_book_id_to_index = {book_id: i for i, book_id in enumerate(token_book_ids)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 40000\n",
    "\n",
    "# Less than 1% of paragraphs contain more than 160 tokens.\n",
    "n_tokens = 160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_paragraph_tokens = []\n",
    "for paragraph_tokens in text_paragraph_tokens:\n",
    "    for tokens in paragraph_tokens:\n",
    "        all_paragraph_tokens.append(tokens)\n",
    "len(all_paragraph_tokens), all_paragraph_tokens[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(all_paragraph_tokens)\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, embedding_matrix = paragraph_rnn.get_embedding(tokenizer, GLOVE_100_PATH, max_words)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "dense_size = 64\n",
    "train_emb = True\n",
    "\n",
    "models = []\n",
    "model_weights_fnames = []\n",
    "for category_i, levels in enumerate(category_levels):\n",
    "    category = categories[category_i]\n",
    "    n_classes = len(levels)\n",
    "    model, weights_fname = paragraph_rnn.create_model(category,\n",
    "                                                      n_classes,\n",
    "                                                      n_tokens,\n",
    "                                                      embedding_matrix,\n",
    "                                                      hidden_size,\n",
    "                                                      dense_size,\n",
    "                                                      train_emb=train_emb)\n",
    "    optimizer = keras.optimizers.Adam()\n",
    "    model.compile(optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])\n",
    "    \n",
    "    path = os.path.join(MODELS_PATH, weights_fname)\n",
    "    if os.path.exists(path):\n",
    "        model.load_weights(path)\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_array(sequence):\n",
    "    x = np.zeros((n_tokens,), dtype=np.int32)\n",
    "    if len(sequence) > n_tokens:\n",
    "        # Truncate center.\n",
    "        x[:n_tokens//2] = sequence[:n_tokens//2]\n",
    "        x[-n_tokens//2:] = sequence[-n_tokens//2:]\n",
    "    else:\n",
    "        # Pad beginning ('pre').\n",
    "        x[-len(sequence):] = sequence\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_predictions(x):\n",
    "    y_preds_ordinal = [model.predict([[x]], batch_size=1) for model in models]\n",
    "    y_preds = [ordinal.from_multi_hot_ordinal(y_pred_ordinal) for y_pred_ordinal in y_preds_ordinal]\n",
    "    return x, y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate text (book), and paragraph indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_i = 2\n",
    "paragraph_i = 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_next_paragraph():\n",
    "    global text_i\n",
    "    global paragraph_i\n",
    "    \n",
    "    paragraph_i += 1\n",
    "    while paragraph_i == len(train_text_paragraphs[text_i]):\n",
    "        text_i += 1\n",
    "        if text_i == len(train_text_paragraphs):\n",
    "            return False\n",
    "        paragraph_i = 0\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def move_to_previous_paragraph():\n",
    "    global text_i\n",
    "    global paragraph_i\n",
    "\n",
    "    paragraph_i -= 1\n",
    "    while paragraph_i < 0:\n",
    "        text_i -= 1\n",
    "        if text_i < 0:\n",
    "            return False\n",
    "        paragraph_i = len(train_text_paragraphs[text_i]) - 1\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_html = widgets.HTML(value='')\n",
    "\n",
    "category_toggle_buttons = []\n",
    "for category_i, levels in enumerate(category_levels):\n",
    "    level_descriptions = category_descriptions[category_i]\n",
    "    options = []\n",
    "    description = categories[category_i]\n",
    "    tooltips = []\n",
    "    for level_i, level in enumerate(levels):\n",
    "        options.append((rating_names[level_i], level_i))\n",
    "        split_levels = '\\n'.join(level.split('|'))\n",
    "        level_description = level_descriptions[level_i]\n",
    "        split_level_descriptions = '\\n'.join(level_description.split('|'))\n",
    "        tooltips.append('{}\\n\\n{}'.format(split_levels, split_level_descriptions))\n",
    "    toggle_buttons = widgets.ToggleButtons(\n",
    "        options=options,\n",
    "        description=description,\n",
    "        disabled=False,\n",
    "        button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "        tooltips=tooltips\n",
    "    )\n",
    "    category_toggle_buttons.append(toggle_buttons)\n",
    "\n",
    "\n",
    "def on_submit_button_clicked(button):\n",
    "    # Collect the answers.\n",
    "    for category_i, levels in enumerate(category_levels):\n",
    "        y = category_toggle_buttons[category_i].value\n",
    "        train_text_category_labels[text_i][category_i][paragraph_i] = y\n",
    "\n",
    "    moved = move_to_next_paragraph()\n",
    "    if not do_overwrite:\n",
    "        while moved and all([train_text_category_labels[text_i][category_i][paragraph_i] != -1\n",
    "                             for category_i in range(len(categories))]):\n",
    "            moved = move_to_next_paragraph()\n",
    "\n",
    "    if moved:\n",
    "        display_paragraph_interface()\n",
    "    else:\n",
    "        print('Finished training on {:d} books.'.format(len(train_paragraphs)))\n",
    "\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Submit',\n",
    "    disabled=False,\n",
    "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Submit the above values as the categorical maturity rating levels for this paragraph.\\nThen move on to the next paragraph.',\n",
    "    icon='check'\n",
    ")\n",
    "submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "\n",
    "def on_back_button_clicked(button):\n",
    "    moved_back = move_to_previous_paragraph()\n",
    "    if moved_back:\n",
    "        display_paragraph_interface()\n",
    "    else:\n",
    "        print('Cannot move to previous paragraph.')\n",
    "\n",
    "\n",
    "back_button = widgets.Button(\n",
    "    description='Back',\n",
    "    disabled=False,\n",
    "    button_style='warning',\n",
    "    tooltip='Move to the previous paragraph.'\n",
    ")\n",
    "back_button.on_click(on_back_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_paragraph_interface():\n",
    "    # Calculate meta data.\n",
    "    book_id = train_book_ids[text_i]\n",
    "    book_index = book_id_to_index[book_id]\n",
    "    book_title = books_df.iloc[book_index]['title']\n",
    "    book_authors = books_df.iloc[book_index]['authors']\n",
    "    book_y = Y[:, book_index]\n",
    "    \n",
    "    section_i = train_text_section_ids[text_i][paragraph_i]\n",
    "    sections = train_text_sections[text_i]\n",
    "    section = sections[section_i]\n",
    "    paragraph = train_text_paragraphs[text_i][paragraph_i]\n",
    "    \n",
    "    # Clear any previous output in this cell.\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    \n",
    "    # Print meta data.\n",
    "    c_width = 114\n",
    "    print('-'*c_width)\n",
    "    print('{} (book {:d} of {:d})'.format(book_title, text_i + 1, len(train_book_ids)))\n",
    "    print('{}'.format(book_authors))\n",
    "    print()\n",
    "    print('Actual categorical rating levels:')\n",
    "    for category_i, level_i in enumerate(book_y):\n",
    "        category = categories[category_i]\n",
    "        level = category_levels[category_i][level_i]\n",
    "        print('  {:28}: {} ({})'.format(category, rating_names[level_i], level))\n",
    "    print('-'*c_width)\n",
    "    print()\n",
    "    print('What are the categorical maturity rating levels for this paragraph?')\n",
    "    \n",
    "    # Update toggle buttons from existing labels or predictions.\n",
    "    if all([train_text_category_labels[text_i][category_i][paragraph_i] != -1 for category_i in range(len(categories))]):\n",
    "        for category_i, toggle_buttons in enumerate(category_toggle_buttons):\n",
    "            toggle_buttons.value = train_text_category_labels[text_i][category_i][paragraph_i]\n",
    "    elif do_predict and book_id in token_book_id_to_index.keys():\n",
    "        token_book_index = token_book_id_to_index[book_id]\n",
    "        tokens = text_paragraph_tokens[token_book_index][paragraph_i]\n",
    "        train_sequence = tokenizer.texts_to_sequences([tokens])[0]\n",
    "        x_train = get_input_array(train_sequence)\n",
    "        y_preds = get_predictions(x_train)\n",
    "        for category_i, toggle_buttons in enumerate(category_toggle_buttons):\n",
    "            toggle_buttons.value = y_preds[category_i][0]\n",
    "    else:\n",
    "        for toggle_buttons in category_toggle_buttons:\n",
    "            toggle_buttons.value = 0\n",
    "    \n",
    "    # Display toggle buttons.\n",
    "    for toggle_buttons in category_toggle_buttons:\n",
    "        IPython.display.display(toggle_buttons)\n",
    "    \n",
    "    # Display submit button.\n",
    "    IPython.display.display(submit_button)\n",
    "    \n",
    "    print()\n",
    "    print('{} (section {:d} of {:d})'.format(section, section_i + 1, len(sections)))\n",
    "    print()\n",
    "    print('(paragraph {:d} of {:d})'.format(paragraph_i + 1, len(train_text_paragraphs[text_i])))\n",
    "    print('='*c_width)\n",
    "    paragraph_html.value = '<p style=\"font-size:large;margin-left:8em;max-width:36em;\">{}</p>'.format(paragraph)\n",
    "    IPython.display.display(paragraph_html)\n",
    "    print('='*c_width)\n",
    "    \n",
    "    # Display back button.\n",
    "    IPython.display.display(back_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_paragraph_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_i, category_labels in enumerate(train_text_category_labels):\n",
    "    book_id = train_book_ids[text_i]\n",
    "    asin = books_df[books_df['id'] == book_id].iloc[0]['asin']\n",
    "    \n",
    "    for category_i, category in enumerate(categories):\n",
    "        section_paragraph_labels = [[] for _ in range(len(train_text_sections[text_i]))]\n",
    "        for paragraph_i, section_i in enumerate(train_text_section_ids[text_i]):\n",
    "            label = train_text_category_labels[text_i][category_i][paragraph_i]\n",
    "            section_paragraph_labels[section_i].append(label)\n",
    "        fname = folders.FNAME_TEXT_PARAGRAPHS_LABELS_FORMAT.format(category)\n",
    "        path = os.path.join(folders.AMAZON_KINDLE_TEXT_PATH, asin, fname)\n",
    "        paragraph_io.write_formatted_section_paragraph_labels(section_paragraph_labels, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_i, model in enumerate(models):\n",
    "    fname = model_fnames[category_i]\n",
    "    path = os.path.join(MODELS_PATH, fname)\n",
    "    model.save_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
