{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['Accuracy', 'F1 Score', 'Mean Squared Error']\n",
    "category_names = [bookcave.CATEGORY_NAMES[category] for category in bookcave.CATEGORIES] + ['Average']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_category_metrics(path):\n",
    "    with open(path, 'r', encoding='utf-8') as fd:\n",
    "        n_models = int(fd.readline()[:-1])\n",
    "        model_names = []\n",
    "        for m in range(n_models):\n",
    "            model_name = fd.readline()[:-1]\n",
    "            model_names.append(model_name)\n",
    "        model_category_metrics = []\n",
    "        for m in range(n_models):\n",
    "            category_metrics = []\n",
    "            for j in range(len(category_names)):\n",
    "                metrics = []\n",
    "                for i in range(len(metric_names)):\n",
    "                    metrics.append(float(fd.readline()[:-1]))\n",
    "                category_metrics.append(metrics)\n",
    "            model_category_metrics.append(category_metrics)\n",
    "        return model_names, np.array(model_category_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_names, baseline_category_metrics = read_model_category_metrics('ppb.txt')\n",
    "baseline_category_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/paragraph_cnn_max_ordinal/33063788_overall_max-agg.h5\n",
    "# models/paragraph_rnn_max_ordinal/33063789_overall_max-agg.h5\n",
    "# models/paragraph_rnncnn_max_ordinal/33063790_overall_max-agg.h5\n",
    "model_names, model_category_metrics = read_model_category_metrics('pp.txt')\n",
    "model_category_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names = baseline_names + model_names\n",
    "classifier_category_metrics = np.concatenate([baseline_category_metrics, model_category_metrics])\n",
    "classifier_category_metrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_category_metrics_book(path, metric_indices):\n",
    "    with open(path, 'r', encoding='utf-8') as fd:\n",
    "        n_models = int(fd.readline()[:-1])\n",
    "        model_names = []\n",
    "        for m in range(n_models):\n",
    "            model_name = fd.readline()[:-1]\n",
    "            model_names.append(model_name)\n",
    "        model_category_metrics = []\n",
    "        for m in range(n_models):\n",
    "            category_metrics = []\n",
    "            for j in range(len(category_names)):\n",
    "                metrics = []\n",
    "                all_metrics = [float(value.strip()) for value in fd.readline()[:-1].split('|')[1:-1]]\n",
    "                for index in metric_indices:\n",
    "                    metrics.append(all_metrics[index])\n",
    "                category_metrics.append(metrics)\n",
    "            model_category_metrics.append(category_metrics)\n",
    "        return model_names, np.array(model_category_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_indices = [0, 3, 7]  # [Accuracy, F1 Macro, MSE]\n",
    "model_names_book, model_category_metrics_book = read_model_category_metrics_book('pb.txt', metric_indices)\n",
    "model_category_metrics_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_names_book, baseline_category_metrics_book = read_model_category_metrics_book('pbb.txt', metric_indices)\n",
    "baseline_category_metrics_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names_book = baseline_names_book + model_names_book\n",
    "classifier_category_metrics_book = np.concatenate([baseline_category_metrics_book, model_category_metrics_book])\n",
    "classifier_category_metrics_book.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(classifier_values, classifier_names, tick_names, title, ylabel, xlabel=None, legend=False, save_path=None, figsize=(16, 4.8), gap=.15):\n",
    "    plt.figure(figsize=figsize)\n",
    "    ticks = np.arange(len(tick_names))\n",
    "    width = (1 - gap) / len(classifier_values)\n",
    "    for i, values in enumerate(classifier_values):\n",
    "        plt.bar(ticks + i * width - (1 - gap) / 2 + width / 2, values, width=width)\n",
    "    plt.xticks(ticks, tick_names, rotation=-16.875, ha='left')\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    if legend:\n",
    "        plt.legend(classifier_names, loc='upper center', bbox_to_anchor=(.5, -0.25), ncol=5)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(classifier_category_metrics.shape[2]):\n",
    "    title = '{} of All Classifiers over Individual Paragraphs by Category'.format(metric_names[i])\n",
    "    ylabel = metric_names[i]\n",
    "    legend = i == 2\n",
    "    save_path = os.path.join('..', 'figures', 'classifier_category_metrics_{:d}'.format(i))\n",
    "    plot_bar(classifier_category_metrics[:, :, i], classifier_names, category_names, title, ylabel, legend=legend, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(classifier_category_metrics_book.shape[2]):\n",
    "    title = '{} of All Classifiers over Entire Books by Category'.format(metric_names[i])\n",
    "    ylabel = metric_names[i]\n",
    "    legend = i == 2\n",
    "    save_path = os.path.join('..', 'figures', 'classifier_category_metrics_book_{:d}'.format(i))\n",
    "    plot_bar(classifier_category_metrics_book[:, :, i], classifier_names_book, category_names, title, ylabel, legend=legend, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
