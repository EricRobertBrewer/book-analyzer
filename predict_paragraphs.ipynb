{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from classification import baselines, evaluation, ordinal, shared_parameters\n",
    "import folders\n",
    "from sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'paragraph_tokens'\n",
    "subset_ratio = shared_parameters.DATA_SUBSET_RATIO\n",
    "subset_seed = shared_parameters.DATA_SUBSET_SEED\n",
    "min_len = shared_parameters.DATA_PARAGRAPH_MIN_LEN\n",
    "max_len = shared_parameters.DATA_PARAGRAPH_MAX_LEN\n",
    "min_tokens = shared_parameters.DATA_MIN_TOKENS\n",
    "categories_mode = shared_parameters.DATA_CATEGORIES_MODE\n",
    "inputs, Y, categories, category_levels, book_ids, books_df, _, _, categories_df = \\\n",
    "    bookcave.get_data({source},\n",
    "                      subset_ratio=subset_ratio,\n",
    "                      subset_seed=subset_seed,\n",
    "                      min_len=min_len,\n",
    "                      max_len=max_len,\n",
    "                      min_tokens=min_tokens,\n",
    "                      categories_mode=categories_mode,\n",
    "                      return_meta=True)\n",
    "text_source_tokens = list(zip(*inputs[source]))[0]\n",
    "len(text_source_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_locations = []\n",
    "predict_tokens = []\n",
    "predict_source_labels = []\n",
    "for text_i, source_tokens in enumerate(text_source_tokens):\n",
    "    book_id = book_ids[text_i]\n",
    "    asin = books_df[books_df['id'] == book_id].iloc[0]['asin']\n",
    "    category_labels = [bookcave.get_labels(asin, category) for category in categories]\n",
    "    if any(labels is None for labels in category_labels):\n",
    "        continue\n",
    "    for source_i, tokens in enumerate(source_tokens):\n",
    "        source_labels = [labels[source_i] for labels in category_labels]\n",
    "        if any(label == -1 for label in source_labels):\n",
    "            continue\n",
    "        predict_locations.append((text_i, source_i))\n",
    "        predict_tokens.append(tokens)\n",
    "        predict_source_labels.append(source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_true = np.array(predict_source_labels).transpose()\n",
    "Q_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Vectors (Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = shared_parameters.TEXT_MAX_WORDS\n",
    "split = '\\t'\n",
    "tokenizer = Tokenizer(num_words=max_words, split=split)\n",
    "all_locations = []\n",
    "all_sources = []\n",
    "for text_i, source_tokens in enumerate(text_source_tokens):\n",
    "    for source_i, tokens in enumerate(source_tokens):\n",
    "        all_locations.append((text_i, source_i))\n",
    "        all_sources.append(split.join(tokens))\n",
    "tokenizer.fit_on_texts(all_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = shared_parameters.TEXT_N_PARAGRAPH_TOKENS\n",
    "padding = shared_parameters.TEXT_PADDING\n",
    "truncating = shared_parameters.TEXT_TRUNCATING\n",
    "\n",
    "\n",
    "def get_input_sequence(source_tokens, tokenizer, n_tokens, padding='pre', truncating='pre'):\n",
    "    return np.array(pad_sequences(tokenizer.texts_to_sequences([split.join(tokens) for tokens in source_tokens]),\n",
    "                                  maxlen=n_tokens,\n",
    "                                  padding=padding,\n",
    "                                  truncating=truncating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(folders.MODELS_PATH, 'paragraph_cnn', 'ordinal', '32648156_glove300-emb.h5')\n",
    "model = load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_predict = np.array([get_input_sequence([source_tokens], tokenizer, n_tokens, padding, truncating)\n",
    "                      for source_tokens in predict_tokens])\n",
    "Q_pred_ordinal = model.predict(P_predict)\n",
    "Q_pred = [ordinal.from_multi_hot_ordinal(q, threshold=.5) for q in Q_pred_ordinal]\n",
    "len(Q_pred), len(Q_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, category in enumerate(categories):\n",
    "    print()\n",
    "    print(category)\n",
    "    q_true = Q_true[j]\n",
    "    q_pred = Q_pred[j]\n",
    "    confusion, metrics = evaluation.get_confusion_and_metrics(q_true, q_pred)\n",
    "    print(confusion)\n",
    "    print(metrics[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words (count-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=identity,\n",
    "    tokenizer=identity,\n",
    "    analyzer='word',\n",
    "    token_pattern=None,\n",
    "    max_features=max_words,\n",
    "    norm='l2',\n",
    "    sublinear_tf=True)\n",
    "text_tokens = []\n",
    "for source_tokens in text_source_tokens:\n",
    "    all_tokens = []\n",
    "    for tokens in source_tokens:\n",
    "        all_tokens.extend(tokens)\n",
    "    text_tokens.append(all_tokens)\n",
    "X_w = vectorizer.fit_transform(text_tokens)\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = shared_parameters.EVAL_TEST_SIZE  # b\n",
    "test_random_state = shared_parameters.EVAL_TEST_RANDOM_STATE\n",
    "Y_T = Y.transpose()  # (n, c)\n",
    "X_w_train, X_w_test, Y_train_T, Y_test_T = train_test_split(X_w, Y_T, test_size=test_size, random_state=test_random_state)\n",
    "Y_train = Y_train_T.transpose()  # (c, n * (1 - b))\n",
    "Y_test = Y_test_T.transpose()  # (c, n * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_classifiers = []\n",
    "for j, category in enumerate(categories):\n",
    "    y_train = Y_train[j]\n",
    "    k = len(category_levels[j])\n",
    "    classifiers = baselines.fit_ordinal(baselines.create_svm, X_w_train, y_train, k)\n",
    "    category_classifiers.append(classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_w_predict = vectorizer.transform(predict_tokens)\n",
    "Q_w_pred = []\n",
    "for j, classifiers in enumerate(category_classifiers):\n",
    "    p_w_predict = P_w_predict[j]\n",
    "    k = len(category_levels[j])\n",
    "    q_w_pred = baselines.predict_ordinal(classifiers, p_w_predict, k)\n",
    "    Q_w_pred.append(q_w_pred)\n",
    "len(Q_w_pred), len(Q_w_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, category in enumerate(categories):\n",
    "    print()\n",
    "    print(category)\n",
    "    q_true = Q_true[j]\n",
    "    q_w_pred = Q_w_pred[j]\n",
    "    confusion, metrics = evaluation.get_confusion_and_metrics(q_true, q_w_pred)\n",
    "    print(confusion)\n",
    "    print(metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
