{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# correlated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len, max_len = 250, 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, Y, categories, category_levels =\\\n",
    "    bookcave.get_data({'tokens'},\n",
    "                      min_len=min_len,\n",
    "                      max_len=max_len)\n",
    "text_paragraph_tokens = [paragraph_tokens for paragraph_tokens, _ in inputs['tokens']]\n",
    "len(text_paragraph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_tokens = []\n",
    "for paragraph_tokens in text_paragraph_tokens:\n",
    "    all_tokens = []\n",
    "    for tokens in paragraph_tokens:\n",
    "        all_tokens.extend(tokens)\n",
    "    text_all_tokens.append(all_tokens)\n",
    "len(text_all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gram = 1\n",
    "max_gram = 1\n",
    "max_features = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=identity,\n",
    "    tokenizer=identity,\n",
    "    analyzer='word',\n",
    "    token_pattern=None,\n",
    "    ngram_range=(min_gram, max_gram),\n",
    "    max_features=max_features,\n",
    "    norm='l2',\n",
    "    sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(text_all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [Multi Class Text Classification article](https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_terms = []\n",
    "for category_i, category in enumerate(categories):\n",
    "    y = Y[category_i]\n",
    "    scores, pvals = chi2(X, y)\n",
    "    indices = np.argsort(scores)\n",
    "    terms = [(features[indices[-1 - i]], scores[indices[-1 - i]]) for i in range(top_n)]\n",
    "    category_terms.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_i, category in enumerate(categories):\n",
    "    fname = 'words-{:d}_{}-{:d}-{:d}g{:d}-{:d}f-{:d}n.txt'.format(category_i,\n",
    "                                                                  category,\n",
    "                                                                  len(text_all_tokens),\n",
    "                                                                  min_gram,\n",
    "                                                                  max_gram,\n",
    "                                                                  max_features,\n",
    "                                                                  top_n)\n",
    "    path = os.path.join('logs', 'correlated_words', fname)\n",
    "    if not os.path.exists(path):\n",
    "        terms = category_terms[category_i]\n",
    "        with open(path, 'w', encoding='utf-8') as fd:\n",
    "            fd.write('{:d}\\n'.format(len(terms)))\n",
    "            for term, score in terms:\n",
    "                fd.write('{}\\t{}\\n'.format(term, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "category_terms[bookcave.CATEGORY_INDEX_PROFANITY]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
