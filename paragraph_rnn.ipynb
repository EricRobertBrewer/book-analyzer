{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paragraph_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from classification import ordinal, paragraph_rnn\n",
    "from sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PATH = os.path.join('models')\n",
    "GLOVE_100_PATH = os.path.join('..', '..', 'embeddings', 'glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_min_len = 250\n",
    "text_max_len = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_inputs, Y, categories, category_levels, book_ids, books_df, _, _, categories_df =\\\n",
    "    bookcave.get_data({'text'},\n",
    "                      text_source='tokens',\n",
    "                      text_min_len=text_min_len,\n",
    "                      text_max_len=text_max_len,\n",
    "                      return_meta=True)\n",
    "text_paragraph_tokens = [paragraph_tokens for paragraph_tokens, _ in token_inputs['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_locations = []\n",
    "all_tokens = []\n",
    "for text_i, paragraph_tokens in enumerate(text_paragraph_tokens):\n",
    "    for paragraph_i, tokens in enumerate(paragraph_tokens):\n",
    "        all_locations.append((text_i, paragraph_i))\n",
    "        all_tokens.append(tokens)\n",
    "len(all_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words, oov_token='__UNKNOWN__')\n",
    "tokenizer.fit_on_texts(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, embedding_matrix = paragraph_rnn.get_embedding(tokenizer, GLOVE_100_PATH, max_words)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load labels and flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_min_len = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_locations = []\n",
    "train_tokens = []\n",
    "train_paragraph_labels = []\n",
    "for text_i, paragraph_tokens in enumerate(text_paragraph_tokens):\n",
    "    book_id = book_ids[text_i]\n",
    "    asin = books_df[books_df['id'] == book_id].iloc[0]['asin']\n",
    "    category_labels = [bookcave.get_labels(asin, category) for category in categories]\n",
    "    if any(labels is None for labels in category_labels):\n",
    "        continue\n",
    "    for paragraph_i, tokens in enumerate(paragraph_tokens):\n",
    "        paragraph_labels = [labels[paragraph_i] for labels in category_labels]\n",
    "        if any(label == -1 for label in paragraph_labels):\n",
    "            continue\n",
    "        if len(tokens) < tokens_min_len:\n",
    "            continue\n",
    "        train_locations.append((text_i, paragraph_i))\n",
    "        train_tokens.append(tokens)\n",
    "        train_paragraph_labels.append(paragraph_labels)\n",
    "train_locations = np.array(train_locations)\n",
    "train_paragraph_labels = np.array(train_paragraph_labels)\n",
    "len(train_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 160\n",
    "test_size = .25\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_array(sequence):\n",
    "    x = np.zeros((n_tokens,), dtype=np.int32)\n",
    "    if len(sequence) > n_tokens:\n",
    "        # Truncate center.\n",
    "        x[:n_tokens//2] = sequence[:n_tokens//2]\n",
    "        x[-n_tokens//2:] = sequence[-n_tokens//2:]\n",
    "    else:\n",
    "        # Pad beginning ('pre').\n",
    "        x[-len(sequence):] = sequence\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_tokens)\n",
    "P = np.array([get_input_array(sequence) for sequence in train_sequences])\n",
    "P_train, P_test, paragraph_labels_train, paragraph_labels_test =\\\n",
    "    train_test_split(P,\n",
    "                     train_paragraph_labels,\n",
    "                     test_size=test_size,\n",
    "                     random_state=random_state)\n",
    "Q_train, Q_test = paragraph_labels_train.transpose(), paragraph_labels_test.transpose()\n",
    "Q_train.shape, Q_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "dense_size = 32\n",
    "train_emb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "weights_fnames = []\n",
    "for category_i, category in enumerate(categories):\n",
    "    n_classes = len(category_levels[category_i])\n",
    "    model, weights_fname = paragraph_rnn.create_model(category,\n",
    "                                                      n_classes,\n",
    "                                                      n_tokens,\n",
    "                                                      embedding_matrix,\n",
    "                                                      hidden_size,\n",
    "                                                      dense_size,\n",
    "                                                      train_emb=train_emb)\n",
    "    models.append(model)\n",
    "    weights_fnames.append(weights_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "batch_size = 32\n",
    "validation_split = .25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category_i, category in enumerate(categories):\n",
    "    print()\n",
    "    print(category)\n",
    "    n_classes = len(category_levels[category_i])\n",
    "    model = models[category_i]\n",
    "    q_train = Q_train[category_i]\n",
    "    q_train_ordinal = ordinal.to_multi_hot_ordinal(q_train, num_classes=n_classes)\n",
    "    \n",
    "    optimizer = Adam()\n",
    "    model.compile(optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['binary_accuracy', 'categorical_accuracy'])\n",
    "    _ = model.fit(P_train,\n",
    "                  q_train_ordinal,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size,\n",
    "                  validation_split=validation_split)\n",
    "    \n",
    "    q_pred_ordinal = model.predict(P_test)\n",
    "    q_pred = ordinal.from_multi_hot_ordinal(q_pred_ordinal)\n",
    "    q_test = Q_test[category_i]\n",
    "    print('Accuracy: {:.4%}'.format(accuracy_score(q_test, q_pred)))\n",
    "    confusion = confusion_matrix(q_test, q_pred)\n",
    "    print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_paragraph_labels(q_pred):\n",
    "    return max(q_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_book_labels(X, locations, Y, verbose=0):\n",
    "    Y_pred = np.zeros(Y.shape, dtype=np.int32)\n",
    "    for category_i in range(len(Y)):\n",
    "        if verbose:\n",
    "            print('Predicting labels for category {}...'.format(categories[category_i]))\n",
    "        model = models[category_i]\n",
    "        x = X[category_i]\n",
    "        q_pred_ordinal = model.predict(X)\n",
    "        q_pred = ordinal.from_multi_hot_ordinal(q_pred_ordinal)\n",
    "        if verbose:\n",
    "            print('Done.')\n",
    "\n",
    "        # Calculate label for each text.\n",
    "        if verbose:\n",
    "            print('Calculating book labels...')\n",
    "        text_i = locations[0][0]\n",
    "        text_pred = []\n",
    "        for i, pred in enumerate(q_pred):\n",
    "            location = locations[i]\n",
    "            if location[0] != text_i:\n",
    "                label = get_label_from_paragraph_labels(text_pred)\n",
    "                Y_pred[category_i, text_i] = label\n",
    "                text_i = location[0]\n",
    "                text_pred = []\n",
    "            text_pred.append(pred)\n",
    "        label = get_label_from_paragraph_labels(text_pred)\n",
    "        Y_pred[category_i, -1] = label\n",
    "        if verbose:\n",
    "            print('Done.')\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(Y, Y_pred):\n",
    "    for category_i in range(len(Y)):\n",
    "        print()\n",
    "        print(categories[category_i])\n",
    "        y_test, y_pred = Y[category_i], Y_pred[category_i]\n",
    "        print('Accuracy: {:.4%}'.format(accuracy_score(y_test, y_pred)))\n",
    "        confusion = confusion_matrix(y_test, y_pred)\n",
    "        print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on small subset of books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_indices = {text_i for text_i, _ in train_locations}\n",
    "test_locations = []\n",
    "test_tokens = []\n",
    "Y_test = Y[:, list(test_text_indices)]\n",
    "for i, text_i in enumerate(test_text_indices):\n",
    "    for paragraph_i, tokens in enumerate(text_paragraph_tokens[text_i]):\n",
    "        test_locations.append((i, paragraph_i))\n",
    "        test_tokens.append(tokens)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_tokens)\n",
    "X_test = np.array([get_input_array(sequence) for sequence in test_sequences])\n",
    "Y_pred_test = predict_book_labels(X_test, test_locations, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(Y_test, Y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict book ratings for all books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sequences = tokenizer.texts_to_sequences(all_tokens)\n",
    "X_all = np.array([get_input_array(sequence) for sequence in all_sequences])\n",
    "Y_pred_all = predict_book_labels(X_all, all_locations, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(Y, Y_pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
