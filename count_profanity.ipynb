{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count_profanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "from sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len, max_len = 250, 7500\n",
    "categories_mode = 'soft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, Y, categories, category_levels =\\\n",
    "    bookcave.get_data({'tokens'},\n",
    "                      min_len=min_len,\n",
    "                      max_len=max_len,\n",
    "                      categories_mode=categories_mode,\n",
    "                      only_categories={bookcave.CATEGORY_INDEX_PROFANITY})\n",
    "text_paragraph_tokens = [paragraph_tokens for paragraph_tokens, _ in inputs['tokens']]\n",
    "y = Y[0]\n",
    "len(text_paragraph_tokens), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_tokens = []\n",
    "for paragraph_tokens in text_paragraph_tokens:\n",
    "    all_tokens = []\n",
    "    for tokens in paragraph_tokens:\n",
    "        all_tokens.extend(tokens)\n",
    "    text_all_tokens.append(all_tokens)\n",
    "len(text_all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitive terms.\n",
    "terms = [\n",
    "    # Unigrams\n",
    "    'fuck', 'fucking', 'shit', 'ass', 'fucked',\n",
    "    'bitch', 'damn', 'asshole', 'bullshit', 'hell',\n",
    "    'goddamn', 'pissed', 'bastard', 'pussy', 'dick',\n",
    "    'piss', 'cock', 'damned', 'bastards', 'dammit',\n",
    "    'crap', 'whore', 'jesus', 'balls', 'christ',\n",
    "    # Bi-grams\n",
    "    'my god', 'the lord'\n",
    "]\n",
    "# The chi-squared scores as calculated by `correlated_words.ipynb`.\n",
    "scores = [\n",
    "    # Unigrams\n",
    "    53.898025279581404, 52.030154888391806, 47.704418886478464, 34.55475551500284, 33.1162229977325,\n",
    "    30.47933938866534, 29.788814670866852, 29.694698250911742, 27.984428132905137, 23.359744651755545,\n",
    "    23.15129752085929, 22.50285572996259, 21.851090886939872, 20.909696770700794, 17.488034484581796,\n",
    "    17.30660075173425, 17.142074011125963, 15.672210844229596, 13.872836790230444, 12.746700176411284,\n",
    "    9.656894314631861, 9.501671303087658, 3.7650110677763946, 3.09802624783671, 2.8661986785983244,\n",
    "    # Bi-grams\n",
    "    13.953372314896928, 3.4680166228627614\n",
    "]\n",
    "assert(len(terms) == len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    preprocessor=identity,\n",
    "    tokenizer=identity,\n",
    "    analyzer='word',\n",
    "    token_pattern=None,\n",
    "    ngram_range=(1, 2),\n",
    "    vocabulary=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sparse = vectorizer.fit_transform(text_all_tokens)\n",
    "X = X_sparse.toarray()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred(count, mode='soft'):\n",
    "    if mode == 'hard':\n",
    "        thresholds = [0, 5, 40, 100, 200, 500]\n",
    "        profanity_levels = [0, 2, 3, 4, 5, 5, 6]\n",
    "    elif mode == 'medium':\n",
    "        thresholds = [0, 5, 40, 100, 500]\n",
    "        profanity_levels = [0, 2, 3, 4, 5, 6]\n",
    "    elif mode == 'soft':\n",
    "        thresholds = [0, 5, 100]\n",
    "        profanity_levels = [0, 1, 2, 3]\n",
    "    else:\n",
    "        raise ValueError('Unknown value for `mode`: {}'.format(mode))\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        if count <= threshold:\n",
    "            return profanity_levels[i]\n",
    "    return profanity_levels[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(flags):\n",
    "    counts = np.dot(X, np.array(flags))    \n",
    "    y_pred = [get_y_pred(count, mode=categories_mode) for count in counts]\n",
    "\n",
    "    confusion = confusion_matrix(y, y_pred)\n",
    "    print(confusion)\n",
    "    \n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    print('{:>20}: {:.4f}'.format('Accuracy', accuracy))\n",
    "    precision_macro = precision_score(y, y_pred, average='macro')\n",
    "    print('{:>20}: {:.4f}'.format('Precision (macro)', precision_macro))\n",
    "    recall_macro = recall_score(y, y_pred, average='macro')\n",
    "    print('{:>20}: {:.4f}'.format('Recall (macro)', recall_macro))\n",
    "    f1_macro = f1_score(y, y_pred, average='macro')\n",
    "    print('{:>20}: {:.4f}'.format('F1 (macro)', f1_macro))\n",
    "    precision_weighted = precision_score(y, y_pred, average='weighted')\n",
    "    print('{:>20}: {:.4f}'.format('Precision (weighted)', precision_weighted))\n",
    "    recall_weighted = recall_score(y, y_pred, average='weighted')\n",
    "    print('{:>20}: {:.4f}'.format('Recall (weighted)', recall_weighted))\n",
    "    f1_weighted = f1_score(y, y_pred, average='weighted')\n",
    "    print('{:>20}: {:.4f}'.format('F1 (weighted)', f1_weighted))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = [\n",
    "    # Unigrams\n",
    "    1, 1, 1, 1, 1,\n",
    "    1, 0, 1, 1, 0,\n",
    "    1, 1, 1, 1, 1,\n",
    "    1, 1, 1, 1, 1,\n",
    "    0, 0, 0, 0, 0,\n",
    "    # Bi-grams\n",
    "    1, 0\n",
    "]\n",
    "y_pred = print_metrics(flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falses = [x for i, x in enumerate(X) if y[i] == 1 and y_pred[i] == 2]\n",
    "false_totals = np.sum(falses, axis=0)\n",
    "false_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
