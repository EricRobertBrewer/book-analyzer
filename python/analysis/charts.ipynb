{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from python import folders\n",
    "from python.util.evaluation import METRIC_NAMES\n",
    "from python.sites.bookcave import bookcave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = [bookcave.CATEGORY_NAMES[category] for category in bookcave.CATEGORIES] + ['Average']\n",
    "category_to_index = {category: i for i, category in enumerate(bookcave.CATEGORIES)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['Accuracy', 'F1 Macro', 'MSE']\n",
    "\n",
    "\n",
    "def read_model_category_metrics(path):\n",
    "    with open(path, 'r', encoding='utf-8') as fd:\n",
    "        n_models = int(fd.readline()[:-1])\n",
    "        model_names = []\n",
    "        for m in range(n_models):\n",
    "            model_name = fd.readline()[:-1]\n",
    "            model_names.append(model_name)\n",
    "        model_category_metrics = []\n",
    "        for m in range(n_models):\n",
    "            category_metrics = []\n",
    "            for j in range(len(category_names)):\n",
    "                metrics = []\n",
    "                for i in range(len(metric_names)):\n",
    "                    metrics.append(float(fd.readline()[:-1]))\n",
    "                category_metrics.append(metrics)\n",
    "            model_category_metrics.append(category_metrics)\n",
    "        return model_names, np.array(model_category_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_baselines_path = os.path.join(folders.INPUT_PATH, 'predict_paragraphs_baselines.txt')\n",
    "baseline_names, baseline_category_metrics = read_model_category_metrics(paragraphs_baselines_path)\n",
    "baseline_category_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/paragraph_cnn_max_ordinal/33063788_overall_max-agg.h5\n",
    "# models/paragraph_rnn_max_ordinal/33063789_overall_max-agg.h5\n",
    "# models/paragraph_rnncnn_max_ordinal/33063790_overall_max-agg.h5\n",
    "paragraphs_path = os.path.join(folders.INPUT_PATH, 'predict_paragraphs.txt')\n",
    "model_names, model_category_metrics = read_model_category_metrics(paragraphs_path)\n",
    "model_category_metrics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names = baseline_names + model_names\n",
    "classifier_category_metrics = np.concatenate([baseline_category_metrics, model_category_metrics])\n",
    "classifier_category_metrics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_category_metrics_book(path, metric_indices):\n",
    "    with open(path, 'r', encoding='utf-8') as fd:\n",
    "        n_models = int(fd.readline()[:-1])\n",
    "        model_names = []\n",
    "        for m in range(n_models):\n",
    "            model_name = fd.readline()[:-1]\n",
    "            model_names.append(model_name)\n",
    "        model_category_metrics = []\n",
    "        for m in range(n_models):\n",
    "            category_metrics = []\n",
    "            for j in range(len(category_names)):\n",
    "                metrics = []\n",
    "                all_metrics = [float(value.strip()) for value in fd.readline()[:-1].split('|')[1:-1]]\n",
    "                for index in metric_indices:\n",
    "                    metrics.append(all_metrics[index])\n",
    "                category_metrics.append(metrics)\n",
    "            model_category_metrics.append(category_metrics)\n",
    "        return model_names, np.array(model_category_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_indices = [0, 3, 7]  # [Accuracy, F1 Macro, MSE]\n",
    "books_path = os.path.join(folders.INPUT_PATH, 'predict_books.txt')\n",
    "model_names_book, model_category_metrics_book = read_model_category_metrics_book(books_path, metric_indices)\n",
    "model_category_metrics_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_baselines_path = os.path.join(folders.INPUT_PATH, 'predict_books_baselines.txt')\n",
    "baseline_names_book, baseline_category_metrics_book = read_model_category_metrics_book(books_baselines_path, metric_indices)\n",
    "baseline_category_metrics_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_names_book = baseline_names_book + model_names_book\n",
    "classifier_category_metrics_book = np.concatenate([baseline_category_metrics_book, model_category_metrics_book])\n",
    "classifier_category_metrics_book.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(classifier_values, classifier_names, tick_names, title, ylabel, xlabel=None, legend=False, save_path=None, figsize=(16, 4.8), gap=.15):\n",
    "    plt.figure(figsize=figsize)\n",
    "    ticks = np.arange(len(tick_names))\n",
    "    width = (1 - gap) / len(classifier_values)\n",
    "    for i, values in enumerate(classifier_values):\n",
    "        plt.bar(ticks + i * width - (1 - gap) / 2 + width / 2, values, width=width)\n",
    "    plt.xticks(ticks, tick_names, rotation=-16.875, ha='left')\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    if xlabel is not None:\n",
    "        plt.xlabel(xlabel)\n",
    "    if legend:\n",
    "        plt.legend(classifier_names, loc='upper center', bbox_to_anchor=(.5, -0.25), ncol=5)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(classifier_category_metrics.shape[2]):\n",
    "    title = '{} of All Classifiers over Individual Paragraphs by Category'.format(metric_names[i])\n",
    "    ylabel = metric_names[i]\n",
    "    legend = i == 2\n",
    "    save_path = os.path.join(folders.FIGURES_PATH, 'classifier_category_metrics_{:d}'.format(i))\n",
    "    plot_bar(classifier_category_metrics[:, :, i],\n",
    "             classifier_names,\n",
    "             category_names[:],\n",
    "             title, ylabel,\n",
    "             legend=legend,\n",
    "             save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(classifier_category_metrics_book.shape[2]):\n",
    "    title = '{} of All Classifiers over Entire Books by Category'.format(metric_names[i])\n",
    "    ylabel = metric_names[i]\n",
    "    legend = i == 2\n",
    "    save_path = os.path.join('..', 'figures', 'classifier_category_metrics_book_{:d}'.format(i))\n",
    "    plot_bar(classifier_category_metrics_book[:, :, i],\n",
    "             classifier_names_book,\n",
    "             category_names,\n",
    "             title,\n",
    "             ylabel,\n",
    "             legend=legend,\n",
    "             save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatches = ['/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average(metric_values, names, titles, ylabels, xlabels=None, save_path=None, figsize=(12, 4.8)):\n",
    "    fig, axes = plt.subplots(1, len(metric_values), figsize=figsize)\n",
    "    for i, values in enumerate(metric_values):\n",
    "        for j, value in enumerate(values):\n",
    "            axes[i].bar(j, value, color='w', hatch=hatches[7 * j % len(hatches)], edgecolor='k')\n",
    "        axes[i].set_title(titles[i])\n",
    "        axes[i].set_xticklabels([' ']*len(values))\n",
    "        axes[i].set_ylabel(ylabels[i])\n",
    "        if xlabels is not None:\n",
    "            axes[i].set_xlabel(xlabels[i])\n",
    "#     fig.legend(names, loc='upper center', bbox_to_anchor=(.5, -0.25), ncol=5)\n",
    "    legend = fig.legend(names, loc='lower center', bbox_to_anchor=(.52, -.0025), ncol=5, fontsize='medium')\n",
    "#     fig.legend(names, loc='center right', bbox_to_anchor=(1.35, .5), fontsize='xx-large', markerscale=8)\n",
    "    for i, patch in enumerate(legend.get_patches()):\n",
    "        patch.set_y(patch.get_y() - patch.get_height() / 2 - (patch.get_height() * .1 if i % 2 == 1 else 0))\n",
    "        patch.set_height(patch.get_height() * 2)\n",
    "    fig.tight_layout()\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average([classifier_category_metrics[:, -1, 0], classifier_category_metrics[:, -1, 1]],\n",
    "             classifier_names,\n",
    "             ['Classification Accuracy for Paragraphs', 'Macro-averaged F1 Score for Paragraphs'],\n",
    "             ['Accuracy', 'F1 Score'],\n",
    "             xlabels=[' ', ' '],\n",
    "             save_path=os.path.join(folders.FIGURES_PATH, 'results_average_paragraph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average([classifier_category_metrics_book[:, -1, 0], classifier_category_metrics_book[:, -1, 1]],\n",
    "             classifier_names_book,\n",
    "             ['Classification Accuracy for Books', 'Macro-averaged F1 Score for Books'],\n",
    "             ['Accuracy', 'F1 Score'],\n",
    "             xlabels=[' ', ' '],\n",
    "             save_path=os.path.join(folders.FIGURES_PATH, 'results_average_book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a LaTeX table with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(2):\n",
    "    for j, category in enumerate(category_names):\n",
    "        best_score = max(classifier_category_metrics_book[:, j, m])\n",
    "        print('\\multicolumn{{1}}{{|r|}}{{{}}}'.format(category.replace('&', '\\\\&')), end='')\n",
    "        for i, classifier in enumerate(classifier_names_book):\n",
    "            score = classifier_category_metrics_book[i, j, m]\n",
    "            bold_start = '\\\\underline{' if score == best_score else ''\n",
    "            bold_end = '}' if score == best_score else ''\n",
    "            print(' & {}{:.4f}{}'.format(bold_start, score, bold_end), end='')\n",
    "        print('\\\\\\\\')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(2):\n",
    "    for j, category in enumerate(category_names):\n",
    "        best_score = max(classifier_category_metrics[:, j, m])\n",
    "        print('{}'.format(category.replace('&', '\\\\&')), end='')\n",
    "        for i, classifier in enumerate(classifier_names):\n",
    "            score = classifier_category_metrics[i, j, m]\n",
    "            bold_start = '\\\\underline{' if score == best_score else ''\n",
    "            bold_end = '}' if score == best_score else ''\n",
    "            print(' & {}{:.4f}{}'.format(bold_start, score, bold_end), end='')\n",
    "        print('\\\\\\\\')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
